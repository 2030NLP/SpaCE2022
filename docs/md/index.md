
<br/>

# 第二届中文空间语义理解评测<small>（SpaCE2022）</small>

空间范畴是人类认知中重要的基础范畴。理解文本中的空间信息不仅需要掌握字词含义，还需要具有常识或背景知识，要调动语言范畴之外的空间想象等认知能力。

空间语义理解在NLP领域也长期受到关注，是NLP评测的重要内容之一，但以往相关评测任务主要关注语言中正确的空间语义信息的分析。人类在能够识别常规、正确的空间信息的同时，还能够识别异常、错误的空间信息。如对于“在四面签一个名字”，人类能够意识到其中存在异常，因为“一个名字”通常不会签在“四面”；又如“跳进山洞外”，“跳进”搭配的必须是表达一个空间内部方位的成分，如“山洞中、山洞里”，无法搭配“山洞外”。显然，空间方位表达的异常存在不同的类型，如词语搭配问题、上下文语义冲突问题、违反常识或背景信息的问题等。

基于以上认识，我们于2021年依托 <a target="_blank" href="http://cips-cl.org/static/CCL2021/cclEval/taskEvaluation/index.html">CCL2021</a> 成功举办了首届中文空间语义理解评测任务（SpaCE，Spatial Cognition Evaluation）。

今年，我们依托 <a href="http://cips-cl.org/static/CCL2022/index.html" target="_blank">CCL2022</a> ，继续推出<span style="color:var(--notice-red)">**第二届中文空间语义理解评测（SpaCE2022）**</span>。

- 主办单位：北京大学
- 组织者：詹卫东，穗志方
- 工作人员：孙春晖，李楠，邢丹，王诚文，岳朋雪，王希豪，邱晓枫 等
- 联系方式：sc_eval@163.com

<a target="_blank" href="https://2030nlp.github.io/SpaCE2022/register"><span style="color:var(--notice-red)">👉 **点我立即报名** 👈</span></a>

> - [任务简介](#intro)
> - [数据概况](#data-overview)
>   - [子任务1：中文空间语义正误判断](sub-task-1)
>   - [子任务2：中文空间语义异常归因](sub-task-2)
>   - [子任务3：中文空间实体识别与空间方位关系标注任务](sub-task-3)
> - [评测指标](#eval)
> - [比赛日程](#schedule)
> - [报名方式](#register)
> - [奖项设置](#award)
> - [附录](#appendix)



<br/><span id="intro"></span>

### 1.  任务内容

#### 1.1  任务简介

空间范畴是人类认知中重要的基础范畴。理解文本中的空间信息不仅需要掌握字词含义，还需要具有常识或背景知识，要调动语言范畴之外的空间想象等认知能力。

空间语义理解在NLP领域也长期受到关注，是NLP评测的重要内容之一，但以往相关评测任务主要关注语言中正确的空间语义信息的分析。人类在能够识别常规、正确的空间信息的同时，还能够识别异常、错误的空间信息。如对于“在四面签一个名字”，人类能够意识到其中存在异常，因为“一个名字”通常不会签在“四面”；又如“跳进山洞外”，“跳进”搭配的必须是表达一个空间内部方位的成分，如“山洞中、山洞里”，无法搭配“山洞外”。显然，空间方位表达的异常存在不同的类型，如词语搭配问题、上下文语义冲突问题、违反常识或背景信息的问题等。

基于以上认识，我们于2021年首次提出中文空间语义理解评测任务（SpaCE，Spatial Cognition Evaluation）。在SpaCE2021的基础上，现组织第二届中文空间语义理解评测（SpaCE2022），分为如下3个子任务：

**子任务1，中文空间语义正误判断**：判断给定的中文文本中是否存在空间语义异常。

**子任务2，中文空间语义异常归因**：识别给定中文文本中空间语义异常的片段及其类型。

**子任务3，中文空间实体识别与空间方位关系标注任务**：基于给定的空间关系标注规范，对给定中文文本进行空间实体的识别与空间方位关系标注。

#### 1.2  与 SpaCE2021 的比较

SpaCE2022 与 SpaCE2021 相比，有如下变化：

- ① 使用新的语料资源，制作了<span style="color:var(--notice-red)">**全新**</span>的数据集，扩大了数据规模。SpaCE2022 在语料筛选上更加严格，增加了更多更适用于空间语义理解的语料（来自体育、地理、交通等领域），减少了不适合的语料（如武侠小说、散文等）。同时，在制作数据集时引入了更严格的质量控制机制，提高了数据质量。
- ② 调整了子任务2（归因任务）的形式。SpaCE2021 中以判断题的形式来评估机器对空间异常的归因能力，其评估意义不够充分、直接；SpaCE2022 要求参赛系统直接给出存在异常的文本片段及归因类型，能够更直观地体现其归因能力。同时，也调整了归因任务中的归因类型，从先前的 4 种类型凝练为 3 种，更具归纳性。
- ③ 取消了联合任务，增加了标注任务。SpaCE2021 中的子任务3是前两个任务的联合，因此与前两个任务考察的能力有所重复；SpaCE2022 则引入标注型任务，相较于判断和归因，对参赛系统的空间语义理解能力提出了更高要求，与前两个子任务形成层层递进，从而对参赛系统实现更全方位的考察。

两次评测的比较如下表所示：

|            | SpaCE2021                                                    | SpaCE2022                                                    |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 任务及题型 | 子任务1：中文空间语义正误判断（判断题）<br />子任务2：中文空间语义异常归因合理性判断（判断题）<br />子任务3：中文空间语义判断与归因联合任务（双重判断题） | 子任务1：中文空间语义正误判断（判断题）<br />子任务2：中文空间语义异常归因（<span style="color:var(--notice-red)">填空题+选择题</span>）<br />子任务3：中文空间实体识别与空间方位关系标注任务（<span style="color:var(--notice-red)">标注题</span>） |
| 归因类型   | 搭配问题、语义问题、语境问题、常识问题                       | 搭配不当、语义冲突、不符合常识或背景信息                     |
| 语料来源   | 主要来源于CCL语料库，涵盖小说、散文、词典等文体              | 来源更加丰富，涉及文学、体育、新闻、地理、交通等多种领域     |
| 数据规模   | 7k+ 段原始语料，约 86 万字符                                 | 30k+ 段原始语料，约 300 万字符                               |

> 注：第一届中文空间语义理解评测（SpaCE2021）的相关资源可访问以下链接获取：
>
> - Github 仓库： <a target="_blank" href="https://github.com/2030NLP/SpaCE2021">https://github.com/2030NLP/SpaCE2021</a>
> - 评测网站： <a target="_blank" href="http://ccl.pku.edu.cn:8084/SpaCE2021/">http://ccl.pku.edu.cn:8084/SpaCE2021/</a>
> - CCL2021 评测研讨会资讯： <a target="_blank" href="http://cips-cl.org/static/CCL2021/cclEval/taskEvaluation/index.html">http://cips-cl.org/static/CCL2021/cclEval/taskEvaluation/index.html</a>
<!-- > - Github 仓库： https://github.com/2030NLP/SpaCE2021 -->
<!-- > - 评测网站： http://ccl.pku.edu.cn:8084/SpaCE2021/ -->
<!-- > - 研讨会资讯： http://cips-cl.org/static/CCL2021/cclEval/taskEvaluation/index.html -->



<br/><span id="data-overview"></span>

### 2.  评测数据

#### 2.1  任务要求与数据样例

各子任务的要求说明及样例如下。

<span id="sub-task-1"></span>

##### 2.1.1  子任务1：中文空间语义正误判断

子任务1（中文空间语义正误判断）的数据包含3个部分：① id，试题编号；② context：待判断的文本材料内容；③ judge：对材料空间语义正误的判断结果（正常为 true，异常为 false）。该任务要求参赛系统对于输入的 context 返回 judge 。

样例如下：

```json
// 输入：
"王胡看到十多个送行的男女都在头上扎着白布条，他们哭泣呜咽着走了过来。这些人四面他熟悉的只有阿Q。"

// 输出：
false

// 解释：一般不说“这些人四面”，应该是“这些人中间”或“这些人里面”，故这段话存在空间语义异常。
```



<br/><span id="sub-task-2"></span>

##### 2.1.2  子任务2：中文空间语义异常归因

子任务2（中文空间语义异常归因）的数据主要包含4个部分：① id，试题编号；② context：存在空间语义异常的文本；③ reason：由一或两个文本片段和一个类型标记构成的元组（<text1, [text2,] type>）；④ key：是 context 中对空间语义异常贡献最大的部分，通常为单个词语。该任务要求参赛系统对于输入的 context 返回 reason 元组，其中 text1 或 text2 需要包含 key（但无需专门标记）。

其中，reason 元组涉及以下三种情形：

- **搭配不当**，type 标记为“A”，意味着 text1 与 text2 都是 context 中的文本片段，且通常不会搭配在一起。如` [“空气”, “旁”, “A”]`、`[“地面”, “边”, “A”]` 等。
- **语义冲突**，type 标记为“B”，意味着 text1 与 text2 都是 context 中的文本片段，且在语义上有所矛盾。如 `[“李明把钥匙放在桌下”, “李明捡起桌上的钥匙”, “B”]`、`[“手臂垂直于地面”, “手臂与地面平行”, “B”]` 等。
- **不符合常识或背景信息**，type 标记为“C”，意味着 text1 是 context 中的文本片段，且存在常识性的问题，或者和上下文透露的背景信息不符<!-- ，并由 text2（不限于 context 中的内容）加以解释 -->，如 `[“双手插在裤子口袋外”, “C”]`、`[“脖子旁戴着项圈”, “C”]` 等。

样例如下：

```json
// 输入：
"赵诗人溜回家中吃了午饭，趁着两个处美人在外面逛街，赶紧在沙发旁躺下来睡觉。赵诗人一觉睡到黄昏时刻，两个处美人回来了，见到赵诗人穿着短裤躺在沙发里，几声惊叫才把赵诗人吓醒，他赶紧跳起来，把自己扫地出门。"

// 输出：
["赵诗人在沙发旁躺下来睡觉", "赵诗人穿着短裤躺在沙发里", "B"]

// 解释：这两个文本片段描述的空间关系相互矛盾，因而属于语义冲突类型。其中 key 为“旁”。
```



<br/><span id="sub-task-3"></span>

##### 2.1.3  子任务3：中文空间实体识别与空间方位关系标注任务

子任务3（中文空间实体识别与空间方位关系标注任务）的数据包含3个部分：① id，试题编号；② context：待标注的文本材料内容；③ result，标注结果，包括文本材料中涉及的事件（events）和空间关系（relations）。该任务要求参赛系统基于给定的标注规范，对输入的 context 返回标注后的结果 result 。

样例：

```json
// 输入：
"那少女脸上微微一红，随即现出怒色，将瓷碗往桌上重重一放，转过身去，把铺在房角里的席子、薄被和枕头拿了出来，向房门走去。"

// 输出：
{ "events": [
    {"%放%": {"施事": "少女", "受事": "瓷碗", "终点": "桌##上"}},
    {"%转身%": {"施事": "少女"}},
    {"%铺%": {"受事": "席子、薄被和枕头", "处所": "<在>房角##里"}},
    {"%拿%": {"施事": "少女", "受事": "席子、薄被和枕头"}},
    {"%走%": {"施事": "少女", "方向": "<向>房门"}}],
  "relations": [
    {"#瓷碗#": {"事件": "%放%", "终点": "桌##上"}},
    {"#席子#": {"事件": "%拿%", "起点": "房角", "终点": "少女##手##里"}},
    {"#薄被#": {"事件": "%拿%", "起点": "房角", "终点": "少女##手##里"}},
    {"#枕头#": {"事件": "%拿%", "起点": "房角", "终点": "少女##手##里"}},
    {"#少女#": {"事件": "%走%", "处所": "房##内", "方向": "房门"}},
    {"#席子#": {"事件": "%走%", "起点": "房##内^房角", "方向": "房门"}},
    {"#薄被#": {"事件": "%走%", "起点": "房##内^房角", "方向": "房门"}},
    {"#枕头#": {"事件": "%走%", "起点": "房##内^房角", "方向": "房门"}}]}
```

任务采用的标注规范为北京大学SpaCE2022评测任务课题组提出的《**CSpaceBank_中文文本空间语义信息标注规范**》。

> <span class="fw-bold" style="color:var(--notice-red)">样例仅供参考，实际评测格式可能有所调整。规范文档将择日放出，敬请关注。</span>



<br/>

#### 2.2  数据规模与分布

子任务1~2 共包含约 30k 段语料，按照 7:1:2 的比例分为训练集、验证集、测试集。

子任务3 共包含约 2k 段语料。按照 3:1 的比例分为训练集和测试集。（因数据集规模较小，为保证测试集数据量，不单独提供验证集，参赛者可自行从训练集中划分验证集进行验证。）

语料共约 300 万字符，每段语料字符数均值为 118，标准差为 48.86。语料涉及多种不同类型和来源，各类语料比例为：中小学课本语料（20%）、体育训练语料（6%）、新闻报道语料（37%）、文学作品语料（25%）、地理百科语料（2%）、交通判决书语料（9%），以及其他语料（1%）。



<br/><span id="eval"></span>

### 3.  评价标准

#### 3.1  子任务1的评价标准

子任务1使用准确率（ *`Acc`* ，Accuracy）作为评价指标，公式如下：

```python
Acc = 命中正确答案的题数 / 题目总数
```

#### 3.2  子任务2的评价标准

子任务2按照以下步骤计算得分，作为评价指标：

- ① 首先检查 text1 和 text2 字段中是否包含参考答案认定的 key（`hasKey`），包含则进行后续计算，否则计 0 分；
- ② 根据参考答案的 text1, text2 字段，和参赛者提交的 text1, text2 字段计算 *`F1`* 值，公式为 `F1 = 2 * P * R / (P + R)` ，其中 *`P`* 、 *`R`* 分别代表精确率（Precision）和召回率（Recall）；
- ③ 若归因类型标签错误（`typeError`），则以一定权重（`typeWeight`，<span style="_color:#000">具体数值在数据集发布时给出</span>）扣除少量分值，否则不扣分；
- ④ 对子任务2所有题目的得分计算平均分，作为最终得分。

其中每一道题得分的计算公式如下：

```python
Score = (1 if hasKey else 0) * F1 * ((1-typeWeight) if typeError else 1)
```

#### 3.3  子任务3的评价标准

子任务3在评价时，首先会将参考答案与提交结果转换为边和节点的三元组，然后采用依存句法分析领域常用的 UAS（unlabeled attachment score）和 LAS（labeled attachment score）指标进行评价，以 *`F1`* 值的形式进行计算（公式同子任务2的步骤②）。

在此任务中，为 UAS 计算精确率 *`P`* 时仅要求三元组中的两个节点正确；为 LAS 计算精确率 *`P`* 时则要求三元组中的节点和边完全正确。

#### 3.4  最终排名

在所有参赛队伍的评测结果产生之后，计算每个任务下各个队伍的标准分数（Z-score），对三个任务的标准分数取平均，作为最终排名的依据。标准分数计算公式如下，其中 *`X̄`* 为平均数， *`s`* 为标准差：

```python
Z = (X - X̄) / s
```



<br/><span id="schedule"></span>

### 4.  评测赛程

| 时间 | 事项 |
| :--: | :--: |
| 6月1日~8月20日 | 开放报名 |
| 7月中下旬 | 发布全部子任务的训练集以及子任务1~2的无答案验证集，开放结果提交 |
| 8月5日 | 发布子任务1~2的验证集答案 |
| 9月1日 | 发布无答案的测试集，开始提交最终模型及技术报告 |
| 9月5日 | 测试集结果提交 |
| 9月12日 | 模型及技术报告提交 |
| 9月30日 | 公布结果 |
| 10月14日~10月16日 | 评测研讨会 |

<!-- （以上时间均为暂定，请关注 [CCL 2022](http://cips-cl.org/static/CCL2022/index.html) 官方网站。） -->
（以上时间为暂定，请以实际通知为准。）




<br/><span id="register"></span>

### 5.  报名方式

请仔细阅读《<a target="_blank" href="https://github.com/2030NLP/SpaCE2022/blob/main/Agreement.md">第二届中文空间语义理解评测 SpaCE2022 参赛协议</a>》和《<a target="_blank" href="https://github.com/2030NLP/SpaCE2022/blob/main/data/LICENSE.md">第二届中文空间语义理解评测 SpaCE2022 数据集使用许可</a>》，

然后点击进入 <a target="_blank" href="https://2030nlp.github.io/SpaCE2022/register">报名链接</a> 进行报名。



<br/><span id="award"></span>

### 6.  奖项设置

**评测奖金由华为公司赞助**，奖池共计 50000 元人民币。

一等奖 0-2名 ，奖金合计 20000 元<!-- （如 1*20000, 2*10000） -->

二等奖 0-2名 ，奖金合计 15000 元<!-- （如 1*15000, 2*7500） -->

三等奖 0-4名 ，奖金合计 15000 元<!-- （如 1*15000, 2*7500, 3*5000, 4*3500） -->

由中国中文信息学会为本次评测获奖队伍提供荣誉证书。



<br/><span id="appendix"></span>

### 附录

- <a target="_blank" href="https://github.com/2030NLP/SpaCE2022/blob/main/Agreement.md">第二届中文空间语义理解评测 SpaCE2022 参赛协议</a>
- <a target="_blank" href="https://github.com/2030NLP/SpaCE2022/blob/main/data/LICENSE.md">第二届中文空间语义理解评测 SpaCE2022 数据集使用许可</a>
<!-- - [第二届中文空间语义理解评测 SpaCE2022 参赛协议](https://github.com/2030NLP/SpaCE2022/blob/main/Agreement.md) -->
<!-- - [第二届中文空间语义理解评测 SpaCE2022 数据集使用许可](https://github.com/2030NLP/SpaCE2022/blob/main/data/LICENSE.md) -->

